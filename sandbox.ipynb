{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### GLOBAL VARS ###\n",
    "\n",
    "## AWS ##\n",
    "s3 = boto3.client('s3')\n",
    "bucket1 = 'bootcampjanuary2018-1a'\n",
    "bucket2 = 'bootcampaugust2017-1b'\n",
    "\n",
    "## QUERIES & DB ##\n",
    "# get all table names\n",
    "table_name_query = \"SELECT table_name FROM information_schema.tables WHERE table_type = 'base table'\"\n",
    "\n",
    "# get number of rows in a table\n",
    "get_row_count_query = \"SELECT COUNT(*) FROM Bootcamp.{}\"\n",
    "\n",
    "# get full table\n",
    "get_table = \"SELECT * FROM Bootcamp.{}\"\n",
    "\n",
    "# credentials for connecting to the MySQL db\n",
    "db_creds = json.load(open('hidden/creds.json'))\n",
    "\n",
    "## DATA FRAME VARS ##\n",
    "\n",
    "needed_cols = [\n",
    "    'loan_amnt', 'open_acc', 'pub_rec', 'total_acc', 'inq_last_6mths', 'earliest_cr_line',\n",
    "    'inactive_loans', 'bad_loans', 'pub_rec_zero', 'collections_12_mths_ex_med', 'home_ownership',\n",
    "    'issue_d', 'status', 'grade', 'term', 'funded_amnt', 'funded_amnt', 'funded_amnt_inv', \n",
    "    'initial_list_status', 'sub_grade', 'is_inc_v', 'pymnt_plan', 'purpose', 'mths_since_last_delinq', \n",
    "    'id', 'member_id'\n",
    "]\n",
    "\n",
    "grades = 'ABCDEFG' # loan grades\n",
    "\n",
    "# sub grades\n",
    "sub_grade = [''.join([x[0], str(x[1])]) for x in product(grades, range(1,6))]\n",
    "\n",
    "# map regions\n",
    "regions_num = {\n",
    "    1: 'NE', \n",
    "    2: 'SE', \n",
    "    3: 'NC', \n",
    "    4: 'NW', \n",
    "    5: 'SW'\n",
    "}\n",
    "\n",
    "regions = {\n",
    "    'NE': ['ME', 'NA', 'VT', 'MA', 'RI', 'CT', 'NJ', 'PA', 'MD', 'VA', \n",
    "           'WV', 'KY', 'OH', 'IN', 'NY', 'NH', 'DC', 'DE'],\n",
    "    'SE': ['NC', 'SC', 'GA', 'FL', 'AL', 'TN', 'MS', 'AR', 'LA', 'OK', 'TX'],\n",
    "    'NC': ['ND', 'SD', 'NE', 'KS', 'MO', 'IA', 'MN', 'WI', 'MI', 'IL'],\n",
    "    'NW': ['WA', 'OR', 'ID', 'MT', 'WY', 'AK'],\n",
    "    'SW': ['CA', 'NV', 'UT', 'CO', 'NM', 'AZ', 'HI']\n",
    "}\n",
    "\n",
    "regionsFix = {}\n",
    "\n",
    "for key in regions.keys():\n",
    "    for state in regions[key]:\n",
    "        regionsFix[state] = key\n",
    "\n",
    "purposes = [\"car\", \"credit_card\", \"other\", \"house\", \"debt_consolidation\",\n",
    "                \"home_improvement\", \"small_business\", \"medical\", \"vacation\",\n",
    "                \"moving\", \"wedding\", \"major_purchase\"]\n",
    "\n",
    "def getDBCursor(creds):\n",
    "    \"\"\"\n",
    "    Get the cursor for the db to perform transaction.\n",
    "\n",
    "    Input: creds (dictionary of login credentials)\n",
    "    Output: MySQL Connection, MySQl cursor object\n",
    "    \"\"\"\n",
    "    cnx = mysql.connector.connect(**creds)\n",
    "    return cnx, cnx.cursor()\n",
    "\n",
    "def queryDB(cursor, query):\n",
    "    \"\"\"\n",
    "    Execute a sql query.\n",
    "\n",
    "    Input: cursor (MySQL cursor object), query (string of sql query)\n",
    "    Output: dictionary containing the lines of the query\n",
    "    \"\"\"\n",
    "    cursor.execute(query)\n",
    "    return cursor.fetchall()\n",
    "\n",
    "def getAllYearsData(conn, start_year, end_year):\n",
    "    \"\"\"\n",
    "    Get a dataframe with all the years data.\n",
    "    \n",
    "    Input: conn (db connection), start_year (int), end_year int)\n",
    "    Output: df (dataframe)\n",
    "    \"\"\"\n",
    "    if start_year > end_year:\n",
    "        t = start_year\n",
    "        start_year = end_year\n",
    "    df = pd.DataFrame()\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        df = pd.concat([df, pd.read_sql('SELECT * FROM Bootcamp.{}_Data'.format(str(year)), con=conn)])\n",
    "    return df\n",
    "\n",
    "def getMemberInfo(conn):\n",
    "    \"\"\"\n",
    "    Get the membership information table in a dataframe\n",
    "    \n",
    "    Input: conn (db connection)\n",
    "    Output: dataframe\n",
    "    \"\"\"\n",
    "    return pd.read_sql('SELECT * FROM Bootcamp.Member_Information', con=conn)\n",
    "    \n",
    "def getBootcampData(creds, start_year, end_year):\n",
    "    \"\"\"\n",
    "    Input: creds (db credentials)\n",
    "    Output: dataframe with the combined member_id and year info\n",
    "    \"\"\"\n",
    "    if start_year > end_year:\n",
    "        t = start_year\n",
    "        start_year = end_year\n",
    "        end_year = start_year\n",
    "    cnx, curs = getDBCursor(creds)\n",
    "    year_df = getAllYearsData(cnx, start_year, end_year)\n",
    "    mem_df = getMemberInfo(cnx)\n",
    "    cnx.close()\n",
    "    curs.close()\n",
    "    # drop rows without a loan amount (ex. only a subset of the years is requested.)\n",
    "    return pd.merge(year_df, mem_df, on='member_id', how='inner') \n",
    "\n",
    "def executeQuery(creds, query):\n",
    "    \"\"\"\n",
    "    Executes a query by connecting, querying, and closes the db connection\n",
    "\n",
    "    Input: creds (db login credentials), query (string of sql query)\n",
    "    Output: dictionary of query results\n",
    "    \"\"\"\n",
    "    result = None\n",
    "    try:\n",
    "        conn, cursor = getDBCursor(creds)\n",
    "        result = queryDB(cursor, query)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        conn.close()\n",
    "        cursor.close()\n",
    "    return result\n",
    "\n",
    "def getTableRowCount(table):\n",
    "    \"\"\"\n",
    "    Get the number of rows in a given table\n",
    "    \n",
    "    Input: tabel_name (str)\n",
    "    Output: number of rows (int)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return executeQuery(db_creds, get_row_count_query.format(table_name))[0][0]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def getDataFrame(table_name):\n",
    "    \"\"\"\n",
    "    Create a pandas dataframe from a file.\n",
    "    \n",
    "    Input: filename (csv file, str)\n",
    "    Output: pandas dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn, cursor = getDBCursor(db_creds)\n",
    "        result = pd.read_sql_query(get_table.format(table_name), conn)\n",
    "    finally:\n",
    "        conn.close()\n",
    "        cursor.close()\n",
    "    return result\n",
    "    \n",
    "def cleanData(table_name, mem_info):\n",
    "    \"\"\"\n",
    "    Create a pandas dataframe from a file. Clean up the data.\n",
    "    \n",
    "    Input: table_name (str), mem_info (dataframe)\n",
    "    Output: pandas dataframe object\n",
    "    \"\"\"\n",
    "    #df = getBootcampData(db_creds, 2007, 2014).apply(pd.to_numeric, errors='ignore')\n",
    "    df = getDataFrame(table_name)\n",
    "    df = pd.merge(df, mem_info, on='member_id').apply(pd.to_numeric, errors='ignore')\n",
    "    \n",
    "    # check that all the needed columns are present \n",
    "    needed = ','.join(df.columns)\n",
    "    if not all(col in needed for col in needed_cols):\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # delete rows with nulls in these columns\n",
    "    df = dropNanRowsColSpecific(df, ['loan_amnt', 'open_acc', 'pub_rec', 'total_acc', 'inq_last_6mths'])\n",
    "    \n",
    "    # fix dates\n",
    "    for col in list(df.columns.values): \n",
    "        if col.endswith('_d') or col in ['earliest_cr_line']:\n",
    "            df[col] = df[col].apply(lambda x: cleanDate(str(x).strip()))\n",
    "    \n",
    "    # drop unneccessary columns\n",
    "    col_to_drop = ['id', 'loan_status', 'url', 'desc', 'title', 'revol_bal', 'revol_util', \n",
    "                   'policy_code', 'grade_num', 'sub_grade_num', 'mths_since_last_record',\n",
    "                  'collections_12_mths_zero', 'emp_title', 'emp_length',\n",
    "                  'mths_since_last_major_derog', 'mths_since_last_record', 'delinq_2yrs_zero', 'zip_code']\n",
    "    df.drop(col_to_drop, axis=1, inplace=True, errors='ignore')\n",
    "    \n",
    "    # specific fixes for certain columns\n",
    "    df = fixFundedToApplied(df)\n",
    "    df = fixTerm(df)\n",
    "    df = fixInitListStatus(df)\n",
    "    df = fixGrade(df)\n",
    "    df = fixSubGrade(df)\n",
    "    df = fixIsIncV(df)\n",
    "    df = fixDeliquency(df)\n",
    "    df = fixFundedToApplied(df)\n",
    "    df = fixPaymentPlan(df)\n",
    "    df = fixPurpose(df)\n",
    "    df = fixStatus(df)\n",
    "    df = fixIssueDate(df)\n",
    "    df = fixRegion(df)\n",
    "    \n",
    "    # change columns to only have either a 0 or 1\n",
    "    df = fixBinary01(df, 'inactive_loans')\n",
    "    df = fixBinary01(df, 'bad_loans')\n",
    "    df = fixBinary01(df, 'pub_rec_zero')\n",
    "    \n",
    "    # fill nan's with specified default values\n",
    "    nan_fill = {\n",
    "        'collections_12_mths_ex_med': 0,\n",
    "    }\n",
    "    df.fillna(nan_fill, inplace=True)\n",
    "    \n",
    "    df['return'] = df['total_pymnt']/df['funded_amnt'] #% return for each loan\n",
    "    df['profit'] = df['total_pymnt'] - df['funded_amnt'] #calculate profit per loan\n",
    "    \n",
    "    return df\n",
    "\n",
    "def fixRegion(df):\n",
    "    \"\"\"\n",
    "    Add region column. \n",
    "    \n",
    "    Input: df (dataframe)\n",
    "    Output: dataframe\n",
    "    \"\"\"\n",
    "    df['region'] = df['addr_state'].map(regionsFix)\n",
    "    return df\n",
    "\n",
    "def cleanDate(in_date):\n",
    "    \"\"\"\n",
    "    Turn str into date.\n",
    "    \n",
    "    Input: in_date (str)\n",
    "    Output: datetime obj \n",
    "    \"\"\"\n",
    "    if len(in_date) > 8:\n",
    "        return datetime.strptime(in_date[:8], '%Y%m%d')\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def fixIssueDate(df):\n",
    "    \"\"\"\n",
    "    Eleminate impossible dates\n",
    "    \n",
    "    Input: df (dataframe)\n",
    "    Output: dataframe\n",
    "    \"\"\"\n",
    "    return df[df['issue_d'].apply(lambda x:  datetime(2006,12,31) < x < datetime.now())]\n",
    "    \n",
    "def fixStatus(df):\n",
    "    \"\"\"\n",
    "    Strip whitespace.\n",
    "    \n",
    "    Input: df (dataframe)\n",
    "    Output: dataframe\n",
    "    \"\"\"\n",
    "    df['status'] = df['status'].apply(lambda x: x.strip().lower())\n",
    "    ones = ['default', 'charged off']\n",
    "    df['status_binary'] = df['status'].apply(lambda x: 1 if x in ones else 0)\n",
    "    return df\n",
    "\n",
    "def dropNanRowsColSpecific(df, col_to_drop):\n",
    "    \"\"\"\n",
    "    Drop the rows from the df that have nulls in the specified columns.\n",
    "    \n",
    "    Input: df (dataframe), cols_to_drop (list of strings)\n",
    "    Output: df (dataframe)\n",
    "    \"\"\"\n",
    "    relevant_cols = []\n",
    "    for col in col_to_drop:\n",
    "        try:\n",
    "            df[col]\n",
    "            relevant_cols.append(col)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return df.dropna(subset=relevant_cols, how='any')\n",
    "\n",
    "def fixGrade(df):\n",
    "    \"\"\"\n",
    "    Remove grades outside of range a-g\n",
    "    Input: df (dataframe)\n",
    "    Output: dataframe\n",
    "    \"\"\"\n",
    "    df['grade'] = df['grade'].apply(lambda x: x.upper())\n",
    "    return df[df['grade'].isin(list(grades))]\n",
    "\n",
    "def fixTerm(df):\n",
    "    \"\"\"\n",
    "    Drop the month part of the term and cast as int.\n",
    "    \n",
    "    Input: df (dataframe)\n",
    "    Output: df (dataframe)\n",
    "    \"\"\"\n",
    "    df['term'] = df['term'].apply(lambda x: int(float(str(x).split()[0])))\n",
    "    return df\n",
    "\n",
    "def fixFundedToApplied(df):\n",
    "    \"\"\"\n",
    "    If funded > applied amount remove the record.\n",
    "    \n",
    "    Input: df (dataframe)\n",
    "    Output: df (dataframe)\n",
    "    \"\"\"\n",
    "    # only keep the records where they were funded less that they applied for\n",
    "    df = df[df['funded_amnt'] < 2 * df['funded_amnt']]\n",
    "    return df[df['funded_amnt'] >= df['funded_amnt_inv']]\n",
    "\n",
    "def fixInitListStatus(df):\n",
    "    \"\"\"\n",
    "    Remove rows that aren't F or W\n",
    "    \n",
    "    Input: df (dataframe)\n",
    "    Output: df (dataframe)\n",
    "    \"\"\"\n",
    "    df['initial_list_status'] = df['initial_list_status'].apply(lambda x: str(x).upper())\n",
    "    df = df[df['initial_list_status'].isin(['F', 'W'])]\n",
    "    df['initial_list_status'] = df['initial_list_status'].apply(lambda x: 0 if x == 'F' else 1)\n",
    "    return df\n",
    "\n",
    "def fixEmpLength(value):\n",
    "    \"\"\"\n",
    "    Clean employment length, should only contain a numeric integer value\n",
    "    i.e. 10+ years transforms to 10\n",
    "         < 1 transforms to 1\n",
    "         \n",
    "    Input: value of cell (String)\n",
    "    Output: years of employment (Int)\n",
    "    \"\"\"\n",
    "    value = value.strip()\n",
    "    if value.startswith('< 1'):\n",
    "        return 1\n",
    "    if value.startswith('10') and value[2] == '+':\n",
    "        return 10\n",
    "    \n",
    "    splitVal = value.split(' ')\n",
    "    if (splitVal[0].isdigit()):\n",
    "        value = splitVal[0]\n",
    "        \n",
    "    return int(value)\n",
    "\n",
    "def fixSubGrade(df):\n",
    "    \"\"\"\n",
    "    Remove rows that don't have a subgrade A1,A2,A3,A4,A5,....,G1...G4,G5\n",
    "    \n",
    "    Input: df (dataframe)\n",
    "    Output: df (dataframe)\n",
    "    \"\"\"\n",
    "    df['sub_grade'] = df['sub_grade'].apply(lambda x: x.upper())\n",
    "    return df[df['sub_grade'].isin(sub_grade)]\n",
    "    \n",
    "def fixIsIncV(df):\n",
    "    \"\"\"\n",
    "    Remove if row doesn't have enumerated status.\n",
    "    \n",
    "    Input: df (dataframe)\n",
    "    Output: df (dataframe)\n",
    "    \"\"\"\n",
    "    df['is_inc_v'] = df['is_inc_v'].apply(lambda x: x.lower())\n",
    "    return df[df['is_inc_v'].isin([\"verified\", \"source verified\", \"not verified\"])]\n",
    "\n",
    "def fixPaymentPlan(df):\n",
    "    \"\"\"\n",
    "    Only leave boolean (val 'n', 'y').\n",
    "    \n",
    "    Input: df (dataframe)\n",
    "    Output: df (dataframe)\n",
    "    \"\"\"\n",
    "    df['pymnt_plan'] = df['pymnt_plan'].apply(lambda x: x.lower())\n",
    "    df = df[df['pymnt_plan'].isin(['n', 'y'])]\n",
    "    df['pymnt_plan'] = df['pymnt_plan'].apply(lambda x: 0 if x == 'n' else 1)\n",
    "    return df\n",
    "\n",
    "def fixPurpose(df):\n",
    "    \"\"\"\n",
    "    Only leave enumerated values. \n",
    "    \n",
    "    Input: df (dataframe)\n",
    "    Output: df (dataframe)\n",
    "    \"\"\"\n",
    "    # remove rows not in the given list of purposes\n",
    "    df['purpose'] = df['purpose'].apply(lambda x: x.lower())\n",
    "    return df[df['purpose'].isin(purposes)]\n",
    "\n",
    "def fixBinary01(df, col_name):\n",
    "    \"\"\"\n",
    "    Only leave boolean (val 0, 1).\n",
    "    \n",
    "    Input: df (dataframe)\n",
    "    Output: df (dataframe)\n",
    "    \"\"\"\n",
    "    df[col_name] = df[col_name].astype('float', errors='ignore')\n",
    "    return df[df[col_name].isin(range(2))]\n",
    "\n",
    "def toInt(i):\n",
    "    \"\"\"\n",
    "    Convert input to integer.\n",
    "    \n",
    "    Input: i (anything)\n",
    "    Output: i (int), if error return original\n",
    "    \"\"\"\n",
    "    try: \n",
    "        return int(i.strip())\n",
    "    except TypeError:\n",
    "        return i\n",
    "    \n",
    "def fixDeliquency(df):\n",
    "    \"\"\"\n",
    "    If the delinq_2yrs col or the mths_since_last_delinq col has value greater than 1, \n",
    "    make new col with true, else false\n",
    "    \n",
    "    Input: df (dataframe)\n",
    "    Output: df (dataframe)\n",
    "    \"\"\"\n",
    "    df['deliquency'] = df.apply(lambda r: 1 if r['delinq_2yrs'] > 0 or \n",
    "                                (r['mths_since_last_delinq'] <= 24 and r['mths_since_last_delinq'] > 0) else 0, \n",
    "                                axis = 1)\n",
    "    df.drop(['delinq_2yrs', 'last_delinq_none'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def getDBTables():\n",
    "    query = \"SELECT table_name FROM information_schema.tables WHERE table_type = 'base table'\"\n",
    "    tables = [x[0] for x in executeQuery(db_creds, query)]\n",
    "    data_tables = {}\n",
    "    memb_count = executeQuery(db_creds, get_row_count_query.format('Member_Information'))\n",
    "    if memb_count:\n",
    "        memb_count = memb_count[0][0]\n",
    "        if memb_count > 0:\n",
    "            data_tables['Member_Information'] = memb_count\n",
    "\n",
    "    for table in tables:\n",
    "        if 'data' in table.lower():\n",
    "            try:\n",
    "                y_start = table.find('20')\n",
    "                if int(table[y_start:y_start + 4]) < datetime.now().year:\n",
    "                    data_tables[table] = executeQuery(db_creds, get_row_count_query.format(table))[0][0]\n",
    "            except:\n",
    "                pass\n",
    "    return data_tables\n",
    "\n",
    "def moveToDeprecated(table_name):\n",
    "    \"\"\"\n",
    "    Move a file that is in /data to /deprecated.\n",
    "    \n",
    "    Input: table_name (str)\n",
    "    \"\"\"\n",
    "    deprecated_path = 'deprecated/{}_{}.csv'.format(table_name, datetime.now().strftime('%Y_%m_%d'))\n",
    "    current_path = 'data/{}.csv'.format(table_name)\n",
    "    try:\n",
    "        s3.copy_object(Bucket=bucket1, CopySource=bucket1+'/'+current_path, Key=deprecated_path)\n",
    "        s3.delete_object(Bucket=bucket1, Key=current_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def uploadFile(filename, obj, is_json):\n",
    "    \"\"\"\n",
    "    Upload data obj to s3\n",
    "    \n",
    "    Input: filename (str), obj (data source)\n",
    "    \"\"\"\n",
    "    tmp_name = '/tmp/' + filename\n",
    "    if is_json:\n",
    "        with open(tmp_name, 'w+') as outfile:  \n",
    "            json.dump(obj, outfile)\n",
    "    elif isinstance(df, pd.DataFrame):\n",
    "        obj.to_csv(tmp_name)\n",
    "        filename = 'data/' + filename\n",
    "    else: # what were you trying to upload anyway...\n",
    "        return\n",
    "    s3.upload_file(tmp_name, bucket1, filename) # upload\n",
    "    os.remove(tmp_name) # clean up\n",
    "    \n",
    "def s3ToDataFrame(table_name):\n",
    "    obj = s3.get_object(Bucket=bucket1, Key='data/{}.csv'.format(table_name))\n",
    "    io = BytesIO(obj['Body'].read())\n",
    "    return pd.read_csv(io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove:  2007_Data\n",
      "tables joined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:119: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modeling finished\n"
     ]
    }
   ],
   "source": [
    "data_tables = getDBTables() # get table names from mysql\n",
    "new_json = {} # place to hold new json that will be the reference next time\n",
    "\n",
    "# get json with table names\n",
    "obj = s3.get_object(Bucket=bucket1, Key='table_data.json') \n",
    "data_from_json = json.loads(obj['Body'].read().decode(\"utf-8\"))\n",
    "\n",
    "to_add = [] # files to upload to s3\n",
    "files_good_to_go = {}\n",
    "for table in list(data_tables):\n",
    "    if table in data_from_json: # if in both\n",
    "        if data_from_json[table] == data_tables[table]: \n",
    "            # if the size of both tables matches, we're good to go and can just move on\n",
    "            files_good_to_go[table] = data_tables[table]\n",
    "            del data_from_json[table]\n",
    "            del data_tables[table]\n",
    "        else: # move the old to deprecated and add table to to_add\n",
    "            to_add.append(table)\n",
    "            new_json[table] = data_tables[table]\n",
    "    else: # just in MySQL\n",
    "        to_add.append(table)\n",
    "        new_json[table] = data_tables[table]\n",
    "        \n",
    "# data_from_json at this point is just what has already been processed should be removed. \n",
    "for table in data_from_json:\n",
    "    moveToDeprecated(table)\n",
    "\n",
    "member_info = None\n",
    "if 'Member_Information' in to_add:\n",
    "    member_info = getDataFrame('Member_Information')\n",
    "    uploadFile('Member_Information.csv', member_info, False)\n",
    "    files_good_to_go['Member_Information'] = new_json['Member_Information']\n",
    "else: \n",
    "    member_info = s3ToDataFrame('Member_Information')\n",
    "    \n",
    "uploadFile('table_data.json', files_good_to_go, True)\n",
    "json_upload = []\n",
    "# to_add is the tables that need to be cleaned and added \n",
    "for table in to_add:\n",
    "    if not 'data' in table.lower(): # skip items that aren't data files (membership information)\n",
    "        continue\n",
    "    df = cleanData(table, member_info)\n",
    "    filename = '{}.csv'.format(table)\n",
    "    uploadFile(filename, df, False)\n",
    "    # upload json with progress to avoid redoing work if crash\n",
    "    files_good_to_go[table] = new_json[table]\n",
    "    uploadFile('table_data.json', files_good_to_go, True)\n",
    "\n",
    "# join tables here into one df\n",
    "df = None\n",
    "for table in list(files_good_to_go):\n",
    "    if not 'data' in table.lower(): # skip items that aren't data files (membership information)\n",
    "        continue\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        df = pd.concat([df, s3ToDataFrame(table)])\n",
    "    else:\n",
    "        df = s3ToDataFrame(table)\n",
    "        \n",
    "## MODELING ##\n",
    "historic_df = df[~df['status'].isin(['late (31-120 days)','current','in grace period','late (16-30 days)'])]\n",
    "open_df = df[~df['status'].isin(['fully paid','default','charged off'])]\n",
    "\n",
    "# Define predictors and response for feature selection\n",
    "logistic = LogisticRegression()\n",
    "\n",
    "# define response variable\n",
    "y = historic_df['status_binary'].values\n",
    "\n",
    "# Create dummy variables and combine with quantitative into predictors matrix\n",
    "categories = historic_df[['is_inc_v','home_ownership','purpose','addr_state','sub_grade']]\n",
    "categories = pd.get_dummies(categories, dummy_na = True)\n",
    "quant = historic_df[['funded_amnt', 'term', 'int_rate','emp_length_num','dti', 'open_acc','total_acc']]\n",
    "X = pd.concat([quant,categories], axis = 1)\n",
    "\n",
    "# do stats\n",
    "X2 = open_df.drop(['status','member_id','issue_d','last_pymnt_d','next_pymnt_d',\n",
    "                   'last_credit_pull_d','final_d','earliest_cr_line','is_inc_v','home_ownership',\n",
    "                   'addr_state','purpose','sub_grade'],axis = 1)\n",
    "categories2 = open_df[['is_inc_v','home_ownership','purpose','addr_state','sub_grade']]\n",
    "quant2 = open_df[['funded_amnt', 'term', 'int_rate','emp_length_num','dti', 'open_acc','total_acc']]\n",
    "categories2 = pd.get_dummies(categories2, dummy_na = True)\n",
    "X2 = pd.concat([quant2, categories2], axis = 1)\n",
    "\n",
    "# make sure the columns of the df's match\n",
    "hist_cols = set(X.columns)\n",
    "open_cols = set(X2.columns)\n",
    "in_hist = list(hist_cols-open_cols)\n",
    "in_open = list(open_cols- hist_cols)\n",
    "to_add_to_hist = {i: X2.columns[i]for i in sorted(X2.columns.get_loc(col) for col in in_open)}\n",
    "to_add_to_open = {i: X.columns[i]for i in sorted(X.columns.get_loc(col) for col in in_hist)}\n",
    "\n",
    "[X.insert(k, v, 0) for k, v in to_add_to_hist.items()]\n",
    "[X2.insert(k, v, 0) for k, v in to_add_to_open.items()]\n",
    "\n",
    "# Fit a logistic model (without using cross validation)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=62)\n",
    "model = logistic.fit(x_train,y_train)\n",
    "preds = logistic.predict(x_test)\n",
    "\n",
    "\n",
    "#calculate model performance results\n",
    "results = confusion_matrix(y_test,preds)\n",
    "tn = results[1,1]\n",
    "tp = results[0,0]\n",
    "fp = results[0,1]\n",
    "fn = results[1,0]\n",
    "\n",
    "# calculate metrics\n",
    "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "\n",
    "model_metrics = [accuracy, precision, recall]\n",
    "uploadFile('metrics.json', model_metrics, True)\n",
    "\n",
    "defaults = logistic.predict(X2)\n",
    "open_df['predict'] = logistic.predict(X2)\n",
    "\n",
    "# rejoin everything\n",
    "df = pd.concat([open_df, historic_df])\n",
    "nan_fill = {\n",
    "        'predict': -1,\n",
    "    }\n",
    "df.fillna(nan_fill, inplace=True)\n",
    "\n",
    "tmp_name = '/tmp/modeled_data.csv'\n",
    "df.to_csv(tmp_name)\n",
    "s3.upload_file(tmp_name, bucket1, 'modeled_data.csv') # upload\n",
    "os.remove(tmp_name) # clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
